% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the theory writeup on the
%%%% Integrative Model for Parallelism,
%%%% copyright Victor Eijkhout (eijkhout@tacc.utexas.edu) 2014-8
%%%%
%%%% avoid.tex : description of communication avoiding
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt,fleqn]{article}

\input setup

\title{Task Graph Transformations for Latency Tolerance}
\author{Victor Eijkhout\thanks{{\tt
      eijkhout@tacc.utexas.edu}, Texas Advanced Computing Center, The
    University of Texas at Austin}}

\begin{document}
\maketitle

\begin{abstract}
The Integrative Model for Parallelism (IMP) derives a task graph from
a higher level description of parallel algorithms.
In this note we show how task graph transformations can be used to
achieve latency tolerance in the program execution. We give a formal
derivation of the graph transformation, and show through simulation
how latency tolerant algorithms can be faster than the naive execution
in a strong scaling scenario.
\end{abstract}

\section{Motivation}

On clusters the cost of communication can be high relatively to
the cost of computation. Hence, the \indexterm{overlapping computation
  and communication} (also known as \indexterm{latency hiding}) has
long been a goal of parallel programming.
%
On shared memory processors
with explicitly managed \indexterm{scratchpad} memory there is an
equivalent phenomenon: if data can be pushed to the scratchpad well in
advance of it being needed, we now hide the memory, rather than
network, latency.

Various techniques for latency hiding have been used. For instance,
the PETSc library~\cite{GrSm:petsc} splits the matrix-vector product
in local and non-local parts, so that the former can overlap the
communication of the latter.
%
Related, redundant computation in order to avoid communication is an
old idea~\cite{OpJo:improved-ssor}.

In this report we will show that latency hiding, and general latency
tolerance, can be achieved by task graph transformations. For this we
use the formalism of the \ac{IMP}, previously defined
in~\cite{Eijkhout:mathematical2016arxiv}.

There is considerable work in the context of iterative methods for
linear system to mitigate the influence of communication.
\begin{itemize}
\item Reformulation of CG-like methods to reduce the number of inner
  products~\cite{ChGe:sstep,DAzEijRo:ppscicomp,Sa:practicalKrylov,Me:multicg}.
\item Multi-step methods that combine inner products, and can have
  better locality properties~\cite{ChGe:sstep}.
\item
  Overlapping either the preconditioner
  application or the matrix-vector product with a
  collective\cite{dehevo92:acta}. We will give a new variant, based
  on~\cite{Gropp:libraries}, that overlaps both.
\end{itemize}
Recently, the notion of redundant computation was revisited by Demmel
\emph{et al.}~\cite{Demmel2008IEEE:avoiding}, in so-called
`communication avoiding' methods. We will show how such methods
naturally arise in the IMP framework. This will be the main result of
this note.

\section{Communication avoiding}

We explain the basic idea of the `communication avoiding' scheme. This
was originally proposed for iterative methods, such as $s$-step CG; in
the next section we will show that the IMP framework can realize this
in general.

\input gridupdate

\section{A `communication avoiding' framework}

We now show how the above scheme, proposed for iterative methods, can
be applied to general task graphs. This means that we can have a
`communication avoiding compiler', that turns an arbitrary computation
into a communication avoiding one.

\input avoiding

%% \section{Strategies for dealing with inter-process communication}
%% \input avoidstrategies

\begin{comment}
  \section{Simple examples}

  \includegraphics{avoid-o3-b2}

  \begin{multicols}{2}
    \small
\begin{verbatim}
Graph on node 0,
    execution of 1 macro steps:
k1 local execution: 0
  parallel time: 0
k3 receive: 2
k2 local execution: 5
  parallel time: 2
k3 local execution: 4
  parallel time: 2
total parallel time : 1*(0+2+2) = 4
overlap analysis:
  2 tasks to recv, 2 k2 parallel time
\end{verbatim}
\vfill\columnbreak
\begin{verbatim}
Graph on node 1,
    execution of 1 macro steps:
k1 local execution: 0
  parallel time: 0
k3 receive: 2
k2 local execution: 5
  parallel time: 2
k3 local execution: 4
  parallel time: 2
total parallel time : 1*(0+2+2) = 4
overlap analysis:
  2 tasks to recv, 2 k2 parallel time
\end{verbatim}
  \end{multicols}

  \vfill\pagebreak
  \includegraphics{avoid-o4-b3}

  \begin{multicols}{2}
    \small
\begin{verbatim}
Graph on node 0,
    execution of 1 macro steps:
k1 local execution: 1
   parallel time: 1
k3 receive: 3
k2 local execution: 8
   parallel time: 3
k3 local execution: 8
   parallel time: 4
 total parallel time : 1*(1+3+4) = 8
 overlap analysis:
   3 tasks to recv, 3 k2 parallel time
\end{verbatim}
\vfill\columnbreak
\begin{verbatim}
Graph on node 1,
    execution of 1 macro steps:
k1 local execution: 1
   parallel time: 1
k3 receive: 3
k2 local execution: 8
   parallel time: 3
k3 local execution: 8
   parallel time: 4
 total parallel time : 1*(1+3+4) = 8
 overlap analysis:
   3 tasks to recv, 3 k2 parallel time
\end{verbatim}

  \end{multicols}

  \section{Example: split computation of matrix-vector product}
  \input avoidmvp
\end{comment}

\section{Simulation}

\begin{figure}[ht]
  \includegraphics[scale=.15]{nodegraph-1}
  \caption{Depiction of $k_1,k_2,k_3$ sets for a processor doing a 1D
    heat equation.}
  \label{fig:1dk123}
\end{figure}

We have written a simple simulator\footnote{The code can be found in
  the code repository~\cite{IMPcode-repo} under {\tt pocs/avoid}.} that takes a task graph and
identifies the $k_1,k_2,k_3$ sets. In figure~\ref{fig:1dk123} we
illustrate these sets for a one-dimensional case, but we note that the
analysis works on arbitrary task graphs.

\begin{figure}[ht]
  \includegraphics[scale=.6]{strongscale-1000}
  \caption{Runtime as a function of core count for moderate latency}
  \label{fig:lat1000}
\end{figure}
\begin{figure}[ht]
  \includegraphics[scale=.6]{strongscale-10000}
  \caption{Runtime as a function of core count for high latency}
  \label{fig:lat10000}
\end{figure}

We then simulate parallel runtimes by evaluating runtimes for the
following scenario:
\begin{itemize}
\item We use a strong scaling scenario where we have a given problem
  size and partitioning into tasks, as well the number of MPI nodes;
\item We set the ratio of message latency to floating point operation
  as fixed;
\item We evaluate the runtime as a function of the number of threads
  available for the task graph on an MPI node.
\end{itemize}
The prediction is that, with non-negligible latency, blocking will
reduce the running time more than the extra computation time from the
extended halo. Also, this effect will be more pronounced as the core
count increases, since this reduces the no-node computation time.

First, in figure~\ref{fig:lat1000} we use a low latency; we see that
only for very high thread count is there any gain. In
figure~\ref{fig:lat10000} we use a higher latency, and we see that
even for moderate thread counts blocking effects latency hiding.


\section{Conclusion}

We have discussed the concepts and prior work in latency hiding and
communication avoiding. We have shown that the IMP framework can turn
an arbitrary computation graph into a communication avoiding one by
judicious partitioning of the task graph, and duplicating certain tasks.

\bibliography{vle,imp}
\bibliographystyle{plain}

\end{document}
